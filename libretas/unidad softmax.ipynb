{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img src=\"http://www.identidadbuho.uson.mx/assets/letragrama-rgb-150.jpg\" width=\"400\" align=\"left\"><img src=\"imagenes/logoLCCazul.jpg\" width=\"170\" align=\"right\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Curso de Redes Neuronales](https://curso-redes-neuronales-unison.github.io/Temario/)\n",
    "\n",
    "# Una sola unidad *softmax*\n",
    "\n",
    "[**Julio Waissman Vilanova**](http://mat.uson.mx/~juliowaissman/), 6 de septiembre de 2017.\n",
    "\n",
    "En esta libreta vamos a revisar los aspectos básicos del aprendizaje para una unidad *softmax* de $K$ salidas, sin capas ocultas y usando el criterio de pérdida de entropia en varias clases. El algoritmo es sencillo pero es importante entenderlo bien antes de pasar a cosas más complicadas.\n",
    "\n",
    "Empecemos por inicializar los modulos que vamos a requerir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from IPython.display import Image  # Esto es para desplegar imágenes en la libreta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. La base de datos a utilizar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La unidad *softmax* es el último de los tipos básicos de neuronas de salida que revisaremos. Para ejemplificar su uso, vamos a utilizar una base de datos bastante comun, MNIST. MNIST es una base de datos de digitos escritos a mano, en formato de $20 \\times 20$ pixeles. La base completa puede obtenerse en la página de Yan LeCun (http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "Nosotros en realidad vamos a utilizar una base de datos reducida de la original y con imágenes de calidad más reducida ($16 \\times 16$ pixeles por imagen). Numpy provée un método para guardad objetos tipo numpy en un solo archivo, utilizando el método de compresión *gunzip*. Los datos ya se encuentran preprocesados y empaquetados en un archivo llamado `digitos.npz`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.load(\"datos/digitos.npz\")\n",
    "\n",
    "print(\"Las llaves del diccionario son: \\n{}\".format(data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, `data` es un objeto contenedor de numpy cuyas llaves son `X_valida`, `X_entrena`, `T_valida`, `T_entrena`. Cada una de estas son a su vez objetos tipo ndarray de numpy, los cuales contienen valores de entrada y salida, tantopara entrenamiento como para validación. No se preocupen, esto de entrenamiento y validación lo vamos a ver más adelante en la clase.\n",
    "\n",
    "Cada renglon de x es una imagen *desenrrollada*, esto es los 256 datos de una imágen de $16 \\times 16$ pixeles. Por otra parte, cada renglon de y es un vector de 10 posiciones, donde todos los valores son ceros, salvo uno, que es el que define la clase de la imagen.\n",
    "\n",
    "Para darse una mejor idea, ejecuta el siguiente script varias veces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = data['X_entrena']\n",
    "y = data['T_entrena']\n",
    "\n",
    "a = np.random.randint(0, y.shape[0])\n",
    "\n",
    "print(\"-- x es de dimensiones {}\".format(x.shape))\n",
    "print(\"-- y es de dimensiones {}\".format(y.shape))\n",
    "\n",
    "print(\"\\ny si escogemos la imagen {} veremos\".format(a))\n",
    "\n",
    "plt.imshow(x[a,:].reshape(16,16), cmap=plt.gray())\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"la cual es un {}\".format(list(y[a,:]).index(1)))\n",
    "\n",
    "print(\"\\n\\nY si miramos lo que contiene, veremos que\")\n",
    "print(\"x[a,:] = \\n{}\\ny[a,:] = \\n{}\".format(x[a,:], y[a,:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O bien, ejecuta este script varias veces para ver un grupo grande de imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.arange(y.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "ind = indices[0:100].reshape(10,10)\n",
    "\n",
    "imagen = np.ones((10 * 16 + 4*11, 10 * 16 + 4*11))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        imagen[4 + i * 20: 20 + i * 20, 4 + j * 20: 20 + j * 20] = x[ind[i, j], :].reshape(16,16)\n",
    "        \n",
    "plt.imshow(imagen, cmap=plt.gray())\n",
    "plt.axis('off')\n",
    "plt.title(u\"Ejemplos aleatorios de imágenes a clasificar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, vamos a hacer una función que agregue la columna de unos para la x extendida, y simplificar más adelante su uso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extendida(x):\n",
    "    \"\"\"\n",
    "    Agrega una columna de unos a x\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.c_[np.ones((x.shape[0], 1)), x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Unidad *softmax*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En una unidad *softmax*, en lugar de estimar solamente un vector de parámetros, el valor de salida lo descomponemos en $K$ vectores de salida con valores de 1 (si pertenece a esa clase) o cero (si no pertenece). A esto se le conoce como *dummy variable*. En el ejemplo que tenemos, las clases ya se encuentran de esa forma (por eso la salida es un vector de dimensión 10 donde solo uno es 1 y todos los demás valores son 0).  \n",
    "\n",
    "El problema de aprendizaje para una unidad *softmax* es estimar una matriz de parámetros $\\omega$ tal que:\n",
    "\n",
    "$$\n",
    "\\omega = (\\omega^{(1)}, \\ldots, \\omega^{(K)}) \n",
    "$$\n",
    "\n",
    "donde $\\omega^{(k)} = (\\omega_0^{(k)}, \\ldots, \\omega_n^{(k)})^T$ es el vector columna que parametriza la clase $k$. De esta manera, $\\omega$ es ahora una matriz de dimensión $n+1 \\times K$. El aporte lineal a cada clase de un objeto $x^(i)$ está dado por\n",
    "\n",
    "$$\n",
    "z^{(i)} = \\omega^T x^{(i)},\n",
    "$$\n",
    "\n",
    "el cual es de dimensión $K \\times 1$ (un valor por cada clase). La probabilidad de pertenecer a la clase $k$ respecto al resto de las clases está dada por:\n",
    "\n",
    "$$\n",
    "\\hat{y}_k^{(i)} = softmax(z^{(i)}, k) = \\frac{\\exp(z_k^{(i)})}{\\sum_{j=1}^K \\exp(z_j^{(i)})}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 1. Con esta información, realiza la función *softmax*, de manera que si recibe un ndarray de dimensiones $ T \\times K$ con $T$ vectores, regrese la matriz de mismas dimensiones con el cálculo *logistica* para cada matriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Calculo de la regresión softmax\n",
    "    \n",
    "    @param z: ndarray de dimensión (T, K) donde z[i, :] es el vector de aportes lineales de el objeto i\n",
    "    \n",
    "    @return: un ndarray de dimensión (T, K) donde cada columna es el calculo softmax de su respectivo vector de entrada.\n",
    "    \n",
    "    \"\"\"\n",
    "    #--------------------------------------------------------------------------------\n",
    "    # AGREGA AQUI TU CÓDIGO\n",
    "    #--------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora es necesario implementar la función de pérdida, la cual es la suma de las pérdidas logarítmicas por cada salida de las k regresiones logísticas, la cual puede resumirse como:\n",
    "\n",
    "$$\n",
    "Loss(y, \\hat{y}) = -\\frac{1}{T}\\sum_{i=1}^T \\sum_{k=1}^K y_k^{(i)} \\log(\\hat{y}_k^{(i)}),\n",
    "$$\n",
    "\n",
    "donde $y_k^{(i)}$ es un valor de 0 o 1 dependiendo si el objeto $i$ pertenece a la clase $k$ o no, mientras que $\\hat{y}_k^{(i)}$ es la probabilidad que el objeto $i$ pertenezca a la clase $k$ conociendo $x^{(i)}$ y parametrizado por $\\omega$, \n",
    "$$\n",
    "\\hat{y}^{(i)} = \\Pr(y^{(i)} = k \\ |\\ x^{(i)}; \\omega).\n",
    "$$\n",
    "\n",
    "#### Ejercicio 2. Implementa la función de pérdida de manera relativamente eficiente, utilizando las facilidades que presenta numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perdida(w, x, y):\n",
    "    \"\"\"\n",
    "    Calcula el costo para la regresión softmax parametrizada por theta, \n",
    "    con el conjunto de datos dado por (x, y)\n",
    "    \n",
    "    @param w: ndarray de dimensión (n+1, K) con los parámetros\n",
    "    @param x: ndarray de dimensión (T, n+1) con los datos\n",
    "    @param y: ndarray de dimensión (T, K) con la clase por cada dato\n",
    "    \n",
    "    @return: Un valor flotante\n",
    "    \n",
    "    \"\"\"\n",
    "    T, K = y.shape\n",
    "    n = x.shape[1] - 1\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    # AGREGA AQUI TU CÓDIGO\n",
    "    #--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #--------------------------------------------------------------------------------\n",
    "\n",
    "def test_perdida():\n",
    "    x = np.array([[1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])\n",
    "    y = np.eye(4)\n",
    "\n",
    "    w = np.array([[3, -4, -4],[-1, -1, 3], [.01, 3, -10], [-5, 5, 5]]).T\n",
    "    \n",
    "    assert perdida(w, x, y) < 0.1\n",
    "    return \"Paso la prueba\"\n",
    "    \n",
    "print(test_perdida())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 3. Implementa la función para predecir el valor de $\\hat{y}$ estimada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predice(w, x):\n",
    "    \"\"\"\n",
    "    Prediccion de y_hat a partir de la matriz omega para los valores de x\n",
    "    \n",
    "    @param w: ndarray de dimensión (n+1, K) con los parámetros\n",
    "    @param x: ndarray de dimensión (T, n+1) con los datos\n",
    "\n",
    "    @return: ndarray de dimensión (T, K) con la clase predecida por cada dato (unos y ceros)\n",
    "    \n",
    "    \"\"\"\n",
    "    #--------------------------------------------------------------------------------\n",
    "    # AGREGA AQUI TU CÓDIGO\n",
    "    #--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #--------------------------------------------------------------------------------\n",
    "\n",
    "def prueba_prediccion():\n",
    "    x = np.array([[1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])\n",
    "    y = np.eye(4)\n",
    "\n",
    "    w = np.array([[3, -4, -4],[-1, -1, 3], [.01, 3, -10], [-5, 5, 5]]).T\n",
    "    \n",
    "    assert abs((y - predice(w, x)).sum()) < 1e-12 \n",
    "    print \"Paso la prueba\"\n",
    "    \n",
    "print(prueba_prediccion())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y por último tenemos que implementar el gradiente para poder utilizar los métodos de optimización (ya sea por descenso de gradiente o por algún método de optimización.\n",
    "\n",
    "El gradiente se obtiene a partir de las derivadas parciales:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Loss(y, \\hat{y})}{\\partial \\omega_j^{(k)}} = - \\frac{1}{T} \\sum_{i = 1}^T \\left(y_k^{(i)} -   \\hat{y}_k^{i}\\right) x_j^{(i)},\n",
    "$$\n",
    "\n",
    "Esto se puede resolver en forma matricial como\n",
    "\n",
    "$$\n",
    "\\nabla_\\omega Loss(y, \\hat{y}) = - \\frac{1}{T} X^T (Y - \\hat{Y})\n",
    "$$\n",
    "\n",
    "#### Ejercicio 4. Implementa el gradiente de la manera que menos se dificulte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradiente(w, x, y):\n",
    "    \"\"\"\n",
    "    Calculo del gradiente para el problema de regresión softmax\n",
    "    \n",
    "    @param w: ndarray de dimensión (n+1, K) con los parámetros\n",
    "    @param x: ndarray de dimensión (T, n+1) con los datos\n",
    "    @param y: ndarray de dimensión (T, K) con la clase por cada dato\n",
    "    \n",
    "    @return: Un ndarray de mismas dimensiones que theta\n",
    "    \n",
    "    \"\"\"\n",
    "    #--------------------------------------------------------------------------------\n",
    "    # AGREGA AQUI TU CÓDIGO\n",
    "    #--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "\n",
    "def prueba_gradiente():\n",
    "    x = np.array([[1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])\n",
    "    y = np.eye(4)\n",
    "\n",
    "    w = np.array([[3, -4, -4],[-1, -1, 3], [.01, 3, -10], [-5, 5, 5]]).T\n",
    "    \n",
    "    print(np.abs(g).max())\n",
    "    assert np.abs(g).max() < 0.05\n",
    "    return \"Paso la prueba\"\n",
    "    \n",
    "print(prueba_gradiente())    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora si, ya nos encontramos en capacidad para realizar el aprendizaje en una unidad softmax.\n",
    "\n",
    "#### Ejercicio 5. Implementa la regresión logística utilizando el método de descenso de gradiente. Utiliza el método de aprendizaje programado para la neurona logistica y completa los comentarios de la función de forma que quede bien documentada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dg_softmax(w, x, y, epsilon=None, max_epoch=10000, tol=1e-3, errores=False):\n",
    "    \"\"\"\n",
    "    Descenso de gradiente por lotes para la clasificación softmax\n",
    "    \n",
    "    AGREGA AQUI LA DOCUMENTACIÓN\n",
    "    \n",
    "    \"\"\"\n",
    "    historial = np.zeros((max_epoch)) if errores else None\n",
    "        \n",
    "    for epoch in xrange(max_epoch):\n",
    "        #--------------------------------------------------------------------------------\n",
    "        # AGREGA AQUI TU CÓDIGO\n",
    "        #--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #--------------------------------------------------------------------------------\n",
    "    return w, historial\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero para utilizar el descenso de gradiente hay que ajustar un valor de $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ajusta un valor de epsilon razonable\n",
    "\n",
    "epsilon_prueba = 1\n",
    "\n",
    "T, K = y.shape\n",
    "n = x.shape[1]\n",
    "\n",
    "w = 0.1 * (np.random.random((n + 1, K)) - 0.5)\n",
    "w, loss_hist = dg_softmax(w, extendida(x), y, epsilon=epsilon_prueba, max_epoch=100, errores=True)\n",
    "plt.plot(loss_hist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y para probarlo vamos a aprender a clasificar a los digitos de nuestra base de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = 0.1 * (np.random.random((n + 1, K)) - 0.5)\n",
    "w, e_hist = dg_softmax(w, extendida(x), y, alpha=alfita, max_epoch=1000)\n",
    "\n",
    "print(\"El costo de la solución final es de {}\".format(costo(w, extendida(x), y)))\n",
    "\n",
    "y_estimada = predice(w, extendida(x))\n",
    "errores = np.where(y.argmax(axis=1) == y_estimada.argmax(axis=1), 0, 1)\n",
    "\n",
    "print(\"\\nLos datos utilizados para el aprendizaje mal clasificados son el {}%\".format(100 * errores.mean()))\n",
    "\n",
    "# Esto solo es para hacerla más emocionante\n",
    "x_test = data['X_valida']\n",
    "y_test = data['T_valida']\n",
    "y_estimada_T = predice(theta, extendida(x_test))\n",
    "errores = np.where(y_test.argmax(axis=1) == y_estimada_T.argmax(axis=1), 0, 1)\n",
    "\n",
    "print(\"\\nY con los datos de pureba el error es del {}%\".format(100 * errores.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Será esta la mejor solución? ¿Será una buena solución? Por esto no hay que preocuparse mucho todavía, lo vamos a revisar más adelante en el curso. Se espera con la unidad *softmax* poder clasificar correctamente más del 95% de los datos de entrenamiento. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
